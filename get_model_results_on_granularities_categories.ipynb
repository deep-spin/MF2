{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean and split categories.\n",
    "def clean_and_split(value):\n",
    "    \"\"\"\n",
    "    Clean and split a value into a list of stripped elements.\n",
    "    Returns empty list for NaN or empty values.\n",
    "    \"\"\"\n",
    "    # print(value)\n",
    "    if pd.isna(value):\n",
    "        print(\"Value is NaN\")\n",
    "    if ',' not in value:\n",
    "        return [value.strip()]\n",
    "    else:\n",
    "        value_list = [item.replace(',', '').strip() for item in value.split(',')  ]\n",
    "        value_list = [item for item in value_list if item != '']\n",
    "        return value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = './your_output_directory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the claims csv file.\n",
    "df_claims = pd.read_csv('./claims.csv')\n",
    "\n",
    "# clean the granularity and category values.\n",
    "df_claims['granularity'] = df_claims['granularity'].apply(lambda x: str(x).strip() if pd.notna(x) else x)\n",
    "df_claims['category'] = df_claims['category'].apply(lambda x: clean_and_split(x))\n",
    "nan_indices = df_claims[df_claims['category'].isna()].index.tolist()\n",
    "print(nan_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_df(processed_results):\n",
    "# Convert to DataFrame\n",
    "    rows = []\n",
    "    for movie_id, movie_data in processed_results.items():\n",
    "        for claim_pair_id, claims in movie_data.items():\n",
    "            claim_id = int(claim_pair_id) \n",
    "            if claims['Claim_1'] is None:\n",
    "                value_true_claim = None\n",
    "            else:\n",
    "                value_true_claim = claims['Claim_1'].lower()\n",
    "            if claims['Claim_2'] is None:\n",
    "                value_false_claim = None\n",
    "            else:\n",
    "                value_false_claim = claims['Claim_2'].lower()\n",
    "            \n",
    "            rows.append({\n",
    "                'movie_id': movie_id,\n",
    "                'claim_id': claim_id,\n",
    "                'prediction_true_claim': value_true_claim,\n",
    "                'prediction_false_claim': value_false_claim\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame and sort by movie_id and claim_id\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.sort_values(['movie_id', 'claim_id'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a dict of output directories, we will load the results from each directory\n",
    "# we also define a list of modalities to read from!\n",
    "# modalities=[\"video_and_transcripts\",\"video_only\",\"transcripts_only\",\"synopsis_only\",\"statement_only\",\"synopsis_w_movie_title\"]\n",
    "modalities=[\"video_and_transcripts\"]\n",
    "model_names = [\"gemini-2.5-pro\", \"gpt-4o\",\"qwen-2.5-72b-instruct\",\"internvl3-78b-instruct\",\"llava-video-72b-qwen2\"]\n",
    "# model_names = [\"gemini-2.5-pro\" ]\n",
    "parsed_outputs_directories = {\n",
    "    \"gemini-2.5-pro\": {\n",
    "        \"synopsis_only\": \"./gemini-2.5-pro-preview-03-25/synopsis_only/explanation_free/last-occurrence_parsed_outputs.json\",\n",
    "        \"statement_only\": \"./gemini-2.5-pro-preview-03-25/statement_only/explanation_free/last-occurrence_parsed_outputs.json\",\n",
    "        \"synopsis_w_movie_title\": \"./gemini-2.5-pro-preview-03-25/synopsis_w_movie_title/explanation_free/last-occurrence_parsed_outputs.json\",\n",
    "        \"transcripts_only\": \"./gemini-2.5-pro-preview-03-25/transcripts_only/explanation_free/last-occurrence_parsed_outputs.json\",\n",
    "        \"video_only\": \"./gemini-2.5-pro-preview-03-25/video_only/explanation_free/last-occurrence_parsed_outputs.json\",\n",
    "        \"video_and_transcripts\": \"./gemini-2.5-pro-preview-03-25/video_and_transcripts/explanation_free/last-occurrence_parsed_outputs.json\"\n",
    "    },\n",
    "    \n",
    "}\n",
    "\n",
    "all_dfs = []\n",
    "for model_name in model_names:\n",
    "    for modality in modalities:\n",
    "        print(f\"Processing {model_name} {modality}\")\n",
    "        # Load the parsed outputs\n",
    "        parsed_outputs_dir = parsed_outputs_directories[model_name][modality]\n",
    "        model_outputs_json = json.load(parsed_outputs_dir)\n",
    "        parsed_outputs = convert_to_df(model_outputs_json)\n",
    "        # add a modality column to the parsed outputs\n",
    "        parsed_outputs['modality'] = modality\n",
    "        parsed_outputs['model'] = model_name\n",
    "        # concatenate the parsed outputs with the merged dataframe\n",
    "        parsed_outputs_df = pd.concat([df_claims, parsed_outputs],axis=1)\n",
    "        parsed_outputs_df = parsed_outputs_df.loc[:, ~parsed_outputs_df.columns.duplicated()]\n",
    "        all_dfs.append(parsed_outputs_df)\n",
    "\n",
    "\n",
    "# Concatenate all dataframes\n",
    "final_df = pd.concat(all_dfs, axis=0, ignore_index=True)\n",
    "\n",
    "# Sort the final dataframe by movie_id, claim_id\n",
    "final_df = final_df.sort_values(['movie_id', 'claim_id'])\n",
    "\n",
    "\n",
    "#Current size of the dataframe is:\n",
    "print(\"Number of rows in the dataframe:\", final_df.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the dataframe with the predicted results has predictions for true and false claims in 'Prediction_True_Claim' and 'Prediction_False_Claim' columns respectively. We define the following functions to compute performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_performance_metrics(df):\n",
    "    df=df.reset_index(drop=True)\n",
    "    correct_true_claim_predictions = (df.Prediction_True_Claim == True).sum()\n",
    "    correct_false_claim_predictions = (df.Prediction_False_Claim == False).sum()\n",
    "    pairwise_correct = df.apply(lambda row: row['prediction_true_claim'] == True and row['prediction_false_claim'] == False, axis=1).sum()\n",
    "    pairwise_accuracy = pairwise_correct / len(df)\n",
    "    accuracy = (correct_true_claim_predictions + correct_false_claim_predictions) / (len(df)*2)\n",
    "    total_pairs = len(df)\n",
    "    return {'true_claim_accuracy': correct_true_claim_predictions / total_pairs, 'false_claim_accuracy': correct_false_claim_predictions / total_pairs,'pairwise_accuracy': pairwise_accuracy, 'accuracy': accuracy, 'total_pairs': total_pairs}\n",
    "\n",
    "\n",
    "def compute_performance_per_movie(df):\n",
    "    # Dictionary to store results\n",
    "    results_per_movie = defaultdict(dict)\n",
    "    # Group by Movie_ID to process each movie separately\n",
    "    movie_groups = df.groupby('movie_id') \n",
    "    for movie_id, movie_df in movie_groups:\n",
    "        results_per_movie[movie_id] = compute_performance_metrics(movie_df)\n",
    "    return results_per_movie\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect results per granularity and per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_per_granularity = {}\n",
    "for (model,modality,granularity), group_df in final_df.groupby(['model','modality','granularity']):\n",
    "    results = compute_performance_metrics(group_df)\n",
    "    # Store results in the dictionary\n",
    "    if model not in results_per_granularity:\n",
    "        results_per_granularity[model] = {}\n",
    "    if modality not in results_per_granularity[model]:\n",
    "        results_per_granularity[model][modality] = {}\n",
    "    results_per_granularity[model][modality][granularity] = results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_df = final_df.explode('category')\n",
    "exploded_df = exploded_df.reset_index()\n",
    "\n",
    "results_per_category = {}\n",
    "for (model,modality,category), group_df in exploded_df.groupby(['Model','Modality','category']):\n",
    "    results = compute_performance_metrics(group_df)\n",
    "    if model not in results_per_category:\n",
    "        results_per_category[model] = {}\n",
    "    if modality not in results_per_category[model]:\n",
    "        results_per_category[model][modality] = {}\n",
    "    results_per_category[model][modality][category] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "with open(os.path.join(OUTPUT_DIR, './results_per_category.json'), 'w') as f:\n",
    "        json.dump(results_per_category, f,indent=4) \n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, './results_per_granularity.json'), 'w') as f:\n",
    "        json.dump(results_per_granularity, f,indent=4)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
